{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn Caffe\n",
    "- Neo Xing, 2016/10\n",
    "- **Incomplete...**\n",
    "- Reference\n",
    "    - [Caffe Tutorial](http://caffe.berkeleyvision.org/tutorial/)\n",
    "    - [Brewing Deep Networks With Caffe](https://docs.google.com/presentation/d/1AuiPxUy7-Dgb36Q_SN8ToRrq6Nk0X-sOFmr7UnbAAHI/edit?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Nets, Layers, and Blobs\n",
    "- **the anatomy of a Caffe model**\n",
    "- Caffe defines a net layer-by-layer in its own model schema. The network defines the entire model bottom-to-top from input data to loss.\n",
    "\n",
    "- As data and derivatives flow through the network in the forward and backward passes Caffe stores, communicates, and manipulates the information as blobs: the blob is the standard array and unified memory interface for the framework.\n",
    "- The layer comes next as the foundation of both model and computation. \n",
    "- The net follows as the collection and connection of layers. \n",
    "\n",
    "#### Blobs\n",
    "- Caffe stores and communicates data using blobs. Blobs provide a unified memory interface holding data; e.g., batches of images, model parameters, and derivatives for optimization.\n",
    "\n",
    "- The conventional blob dimensions for batches of image data are number N x channel K x height H x width W. Blob memory is row-major in layout, so the last / rightmost dimension changes fastest. For example, in a 4D blob, the value at index (n, k, h, w) is physically located at index ((n * K + k) * H + h) * W + w.\n",
    "    - Number N is the batch size\n",
    "    - Channel K is the feature dimension, for RGB images K = 3\n",
    "    \n",
    "- a Blob stores two chunks of memories, data and diff. The former is the normal data that we pass along, and the latter is the gradient computed by the network.\n",
    "\n",
    "#### Layer computation and connections\n",
    "- The layer is the essence of a model and the fundamental unit of computation. Most of the types needed for state-of-the-art deep learning tasks are there.\n",
    "\n",
    "- A layer takes input through bottom connections and makes output through top connections.\n",
    "- Each layer type defines three critical computations: setup, forward, and backward.\n",
    "\n",
    "#### Net definition and operation\n",
    "- The net jointly defines a function and its gradient by composition and auto-differentiation.\n",
    "- The net is a set of layers connected in a computation graph – a directed acyclic graph (DAG) to be exact.\n",
    "- The net is defined as a set of layers and their connections in a plaintext modeling language. \n",
    "- The models are defined in plaintext protocol buffer ([Google Protocol Buffer](https://code.google.com/p/protobuf/)) schema (`prototxt`) while the learned models are serialized as binary protocol buffer (binaryproto) `.caffemodel` files.\n",
    "\n",
    "- A simple logistic regression classifier defined by following codes.\n",
    "\n",
    "``` json\n",
    "name: \"LogReg\"\n",
    "layer {\n",
    "  name: \"mnist\"\n",
    "  type: \"Data\"\n",
    "  top: \"data\"\n",
    "  top: \"label\"\n",
    "  data_param {\n",
    "    source: \"input_leveldb\"\n",
    "    batch_size: 64\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"ip\"\n",
    "  type: \"InnerProduct\"\n",
    "  bottom: \"data\"\n",
    "  top: \"ip\"\n",
    "  inner_product_param {\n",
    "    num_output: 2\n",
    "  }\n",
    "}\n",
    "layer {\n",
    "  name: \"loss\"\n",
    "  type: \"SoftmaxWithLoss\"\n",
    "  bottom: \"ip\"\n",
    "  bottom: \"label\"\n",
    "  top: \"loss\"\n",
    "}\n",
    "```\n",
    "<img src=\"http://caffe.berkeleyvision.org/tutorial/fig/logreg.jpg\" width=\"320px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Forward / Backward\n",
    "- **the essential computations of layered compositional models called by `Solver`**\n",
    "\n",
    "#### Forward\n",
    "- The forward pass computes the output given the input for inference, from bottom to top.\n",
    "\n",
    "<img src=\"http://caffe.berkeleyvision.org/tutorial/fig/forward.jpg\" width=\"320px\" alt=\"the forward pass\">\n",
    "\n",
    "\n",
    "#### Backward\n",
    "- The backward pass computes the gradient given the loss for learning, from top to bottom.\n",
    "- The gradient with respect to the rest of the model is computed layer-by-layer through the chain rule. \n",
    "\n",
    "<img src=\"http://caffe.berkeleyvision.org/tutorial/fig/backward.jpg\" width=\"320px\" alt=\"the backward pass\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Loss\n",
    "- **the task to be learned is defined by the loss**\n",
    "- The loss in Caffe is computed by the Forward pass of the network. \n",
    "\n",
    "#### Loss weights\n",
    "- For nets with multiple layers producing a loss, loss weights can be used to specify their relative importance.\n",
    "- Any layer able to backpropagate may be given a non-zero loss_weight.\n",
    "- The final loss in Caffe, then, is computed by summing the total weighted loss over the network.\n",
    "- the above SoftmaxWithLoss layer could be equivalently written as:\n",
    "\n",
    "``` json\n",
    "layer {\n",
    "  name: \"loss\"\n",
    "  type: \"SoftmaxWithLoss\"\n",
    "  bottom: \"pred\"\n",
    "  bottom: \"label\"\n",
    "  top: \"loss\"\n",
    "  loss_weight: 1\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Solver\n",
    "- **the solver coordinates model optimization**\n",
    "\n",
    "- The solver orchestrates model optimization by coordinating the network’s forward inference and backward gradients to form parameter updates that attempt to improve the loss. \n",
    "- The responsibilities of learning are divided between the Solver for overseeing the optimization and generating parameter updates and the Net for yielding loss and gradients.\n",
    "\n",
    "#### Methods, ...\n",
    "- Stochastic Gradient Descent (type: \"SGD\")\n",
    "- AdaDelta (type: \"AdaDelta\")\n",
    "- Adaptive Gradient (type: \"AdaGrad\")\n",
    "- Adam (type: \"Adam\")\n",
    "- Nesterov’s Accelerated Gradient (type: \"Nesterov\")\n",
    "- RMSprop (type: \"RMSProp\")\n",
    "\n",
    "- To use a learning rate policy like this, you can put the following lines somewhere in your solver prototxt file:\n",
    "\n",
    "``` bash\n",
    "base_lr: 0.01     # begin training at a learning rate of 0.01 = 1e-2\n",
    "lr_policy: \"step\" # learning rate policy: drop the learning rate in \"steps\"\n",
    "                  # by a factor of gamma every stepsize iterations\n",
    "gamma: 0.1        # drop the learning rate by a factor of 10\n",
    "                  # (i.e., multiply it by a factor of gamma = 0.1)\n",
    "stepsize: 100000  # drop the learning rate every 100K iterations\n",
    "max_iter: 350000  # train for 350K iterations total\n",
    "momentum: 0.9\n",
    "```\n",
    "\n",
    "#### Scaffolding\n",
    "- The solver scaffolding prepares the optimization method and initializes the model to be learned\n",
    "\n",
    "#### Updating Parameters\n",
    "- The actual weight update is made by the solver then applied to the net parameters in Solver, incorporates any weight decay r(W) and scaled by the learning rate α.\n",
    "\n",
    "#### Snapshotting and Resuming\n",
    "- The solver snapshots the weights and its own state during training in Solver.\n",
    "- The weight snapshots export the learned model while the solver snapshots allow training to be resumed from a given point.\n",
    "- Snapshotting is configured in the solver definition prototxt.\n",
    "\n",
    "``` bash\n",
    "# The snapshot interval in iterations.\n",
    "snapshot: 5000\n",
    "# File path prefix for snapshotting model weights and solver state.\n",
    "# Note: this is relative to the invocation of the `caffe` utility, not the\n",
    "# solver definition file.\n",
    "snapshot_prefix: \"/path/to/model\"\n",
    "# Snapshot the diff along with the weights. This can help debugging training\n",
    "# but takes more storage.\n",
    "snapshot_diff: false\n",
    "# A final snapshot is saved at the end of training unless\n",
    "# this flag is set to false. The default is true.\n",
    "snapshot_after_train: true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Layer Catalogue\n",
    "- **the layer is the fundamental unit of modeling and computation**\n",
    "#### Vision Layers\n",
    "- Vision layers usually take images as input and produce other images as output.\n",
    "\n",
    "##### Convlolution\n",
    "- The Convolution layer convolves the input image with a set of learnable filters, each producing one feature map in the output image.\n",
    "- Sample (as seen in ./models/bvlc_reference_caffenet/train_val.prototxt)\n",
    "``` json\n",
    "layer {\n",
    "  name: \"conv1\"\n",
    "  type: \"Convolution\"\n",
    "  bottom: \"data\"\n",
    "  top: \"conv1\"\n",
    "  # learning rate and decay multipliers for the filters\n",
    "  param { lr_mult: 1 decay_mult: 1 }\n",
    "  # learning rate and decay multipliers for the biases\n",
    "  param { lr_mult: 2 decay_mult: 0 }\n",
    "  convolution_param {\n",
    "    num_output: 96     # learn 96 filters\n",
    "    kernel_size: 11    # each filter is 11x11\n",
    "    stride: 4          # step 4 pixels between each filter application\n",
    "    weight_filler {\n",
    "      type: \"gaussian\" # initialize the filters from a Gaussian\n",
    "      std: 0.01        # distribution with stdev 0.01 (default mean: 0)\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\" # initialize the biases to zero (0)\n",
    "      value: 0\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "##### Pooling\n",
    "- Sample (as seen in ./models/bvlc_reference_caffenet/train_val.prototxt)\n",
    "``` json\n",
    "layer {\n",
    "  name: \"pool1\"\n",
    "  type: \"Pooling\"\n",
    "  bottom: \"conv1\"\n",
    "  top: \"pool1\"\n",
    "  pooling_param {\n",
    "    pool: MAX\n",
    "    kernel_size: 3 # pool over a 3x3 region\n",
    "    stride: 2      # step two pixels (in the bottom blob) between pooling regions\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "##### Local Response Normalization (LRN)\n",
    "- The local response normalization layer performs a kind of “lateral inhibition” by normalizing over local input regions.\n",
    "\n",
    "#### Loss Layers\n",
    "- Loss drives learning by comparing an output to a target and assigning cost to minimize. The loss itself is computed by the forward pass and the gradient w.r.t. to the loss is computed by the backward pass.\n",
    "\n",
    "- Softmax, Layer type: SoftmaxWithLoss\n",
    "- Sum-of-Squares/ Euclidean, Layer type: EuclideanLoss\n",
    "- Hinge/ Margin, Layer type: HingeLoss\n",
    "- Sigmoid Cross-Entropy, Layer type: SigmoidCrossEntropyLoss\n",
    "- Infogain, Layer type: InfogainLoss\n",
    "\n",
    "#### Activation / Neuron Layers\n",
    "- In general, activation / Neuron layers are element-wise operators, taking one bottom blob and producing one top blob of the same size.\n",
    "\n",
    "- ReLU/Rectified-Linear and Leaky-ReLU, Layer type: ReLU\n",
    "- Sigmoid, Layer type: Sigmoid\n",
    "- TanH / Hyperbolic Tangent, Layer type: TanH\n",
    "- Absolute Value, Layer type: AbsVal\n",
    "- Power, Layer type: Power\n",
    "- BNLL, Layer type: BNLL\n",
    "\n",
    "#### Data Layers\n",
    "- Data enters Caffe through data layers: they lie at the bottom of nets.\n",
    "- Common input preprocessing (mean subtraction, scaling, random cropping, and mirroring) is available by specifying `TransformationParameters`.\n",
    "\n",
    "- Database, Layer type: Data\n",
    "- In-Memory, Layer type: MemoryData\n",
    "- HDF5 Input, Layer type: HDF5Data\n",
    "- HDF5 Output, Layer type: HDF5Output\n",
    "- Images, Layer type: ImageData\n",
    "\n",
    "#### Common Layers\n",
    "- Inner Product (Fully Connected layer), Layer type: InnerProduct\n",
    "    - Sample\n",
    "``` json\n",
    "layer {\n",
    "  name: \"fc8\"\n",
    "  type: \"InnerProduct\"\n",
    "  # learning rate and decay multipliers for the weights\n",
    "  param { lr_mult: 1 decay_mult: 1 }\n",
    "  # learning rate and decay multipliers for the biases\n",
    "  param { lr_mult: 2 decay_mult: 0 }\n",
    "  inner_product_param {\n",
    "    num_output: 1000\n",
    "    weight_filler {\n",
    "      type: \"gaussian\"\n",
    "      std: 0.01\n",
    "    }\n",
    "    bias_filler {\n",
    "      type: \"constant\"\n",
    "      value: 0\n",
    "    }\n",
    "  }\n",
    "  bottom: \"fc7\"\n",
    "  top: \"fc8\"\n",
    "}\n",
    "```\n",
    "\n",
    "- Reshape, Layer type: Reshape\n",
    "    - The Reshape layer can be used to change the dimensions of its input, without changing its data.\n",
    "    - Sample\n",
    "``` json\n",
    "  layer {\n",
    "    name: \"reshape\"\n",
    "    type: \"Reshape\"\n",
    "    bottom: \"input\"\n",
    "    top: \"output\"\n",
    "    reshape_param {\n",
    "      shape {\n",
    "        dim: 0  # copy the dimension from below\n",
    "        dim: 2\n",
    "        dim: 3\n",
    "        dim: -1 # infer it from the other dimensions\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "```\n",
    "\n",
    "- Splitting\n",
    "    - The Split layer is a utility layer that splits an input blob to multiple output blobs. This is used when a blob is fed into multiple output layers.\n",
    "\n",
    "- Flattening\n",
    "    - The Flatten layer is a utility layer that flattens an input of shape n * c * h * w to a simple vector output of shape n * (c*h*w)\n",
    "\n",
    "- Concatenation, Layer type: Concat\n",
    "    - The Concat layer is a utility layer that concatenates its multiple input blobs to one single output blob.\n",
    "    - Sample\n",
    "``` json\n",
    "layer {\n",
    "  name: \"concat\"\n",
    "  bottom: \"in1\"\n",
    "  bottom: \"in2\"\n",
    "  top: \"out\"\n",
    "  type: \"Concat\"\n",
    "  concat_param {\n",
    "    axis: 1\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "- Slicing\n",
    "    - The Slice layer is a utility layer that slices an input layer to multiple output layers along a given dimension (currently num or channel only) with given slice indices.\n",
    "    - Sample\n",
    "``` json\n",
    "layer {\n",
    "  name: \"slicer_label\"\n",
    "  type: \"Slice\"\n",
    "  bottom: \"label\"\n",
    "  ## Example of label with a shape N x 3 x 1 x 1\n",
    "  top: \"label1\"\n",
    "  top: \"label2\"\n",
    "  top: \"label3\"\n",
    "  slice_param {\n",
    "    axis: 1\n",
    "    slice_point: 1\n",
    "    slice_point: 2\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "- Elementwise Operations\n",
    "    - Eltwise\n",
    "    - Argmax\n",
    "    - Softmax\n",
    "    - Mean-Variance Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data\n",
    "- **how to caffeinate data for model input**\n",
    "\n",
    "- Data layers load input and save output by converting to and from Blob to other formats. \n",
    "- Common transformations like mean-subtraction and feature-scaling are done by data layer configuration. \n",
    "- Data and Label\n",
    "    - Data and Label: a data layer has at least one top canonically named data. For ground truth a second top can be defined that is canonically named label.\n",
    "- Transformation\n",
    "    - data preprocessing is parametrized by transformation messages within the data layer definition.\n",
    "- Multiple Inputs\n",
    "    - a Net can have multiple inputs of any number and type. \n",
    "    - Define as many data layers as needed giving each a unique name and top. Multiple inputs are useful for non-trivial ground truth: one data layer loads the actual data and the other data layer loads the ground truth in lock-step. In this arrangement both data and label can be any 4D array. \n",
    "\n",
    "- This data layer definition loads the MNIST digits.\n",
    "``` json\n",
    "layer {\n",
    "  name: \"mnist\"\n",
    "  # Data layer loads leveldb or lmdb storage DBs for high-throughput.\n",
    "  type: \"Data\"\n",
    "  # the 1st top is the data itself: the name is only convention\n",
    "  top: \"data\"\n",
    "  # the 2nd top is the ground truth: the name is only convention\n",
    "  top: \"label\"\n",
    "  # the Data layer configuration\n",
    "  data_param {\n",
    "    # path to the DB\n",
    "    source: \"examples/mnist/mnist_train_lmdb\"\n",
    "    # type of DB: LEVELDB or LMDB (LMDB supports concurrent reads)\n",
    "    backend: LMDB\n",
    "    # batch processing improves efficiency.\n",
    "    batch_size: 64\n",
    "  }\n",
    "  # common data transformations\n",
    "  transform_param {\n",
    "    # feature scaling coefficient: this maps the [0, 255] MNIST data to [0, 1]\n",
    "    scale: 0.00390625\n",
    "  }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Interfaces\n",
    "- **command line, Python, and MATLAB Caffe**\n",
    "- Caffe has command line, Python, and MATLAB interfaces for day-to-day usage, interfacing with research code, and rapid prototyping. \n",
    "\n",
    "#### Command Line\n",
    "- The command line interface – cmdcaffe – is the caffe tool for model training, scoring, and diagnostics.\n",
    "- This tool and others are found in caffe/build/tools. \n",
    "\n",
    "- Training: caffe train learns models from scratch, resumes learning from saved snapshots, and fine-tunes models to new data and tasks:\n",
    "    - All training requires a solver configuration through the -solver solver.prototxt argument.\n",
    "    - Resuming requires the -snapshot model_iter_1000.solverstate argument to load the solver snapshot.\n",
    "    - Fine-tuning requires the -weights model.caffemodel argument for the model initialization.\n",
    "    - For example, you can run:\n",
    "``` bash\n",
    "# train LeNet\n",
    "caffe train -solver examples/mnist/lenet_solver.prototxt\n",
    "# train on GPU 2\n",
    "caffe train -solver examples/mnist/lenet_solver.prototxt -gpu 2\n",
    "# resume training from the half-way point snapshot\n",
    "caffe train -solver examples/mnist/lenet_solver.prototxt -snapshot examples/mnist/lenet_iter_5000.solverstate\n",
    "```\n",
    "- Testing: caffe test scores models by running them in the test phase and reports the net output as its score. \n",
    "    - The net architecture must be properly defined to output an accuracy measure or loss as its output. \n",
    "    - The per-batch score is reported and then the grand average is reported last.\n",
    "``` bash\n",
    "# score the learned LeNet model on the validation set as defined in the\n",
    "# model architeture lenet_train_test.prototxt\n",
    "caffe test -model examples/mnist/lenet_train_test.prototxt -weights examples/mnist/lenet_iter_10000.caffemodel -gpu 0 -iterations 100\n",
    "```\n",
    "\n",
    "- Benchmarking: caffe time benchmarks model execution layer-by-layer through timing and synchronization. \n",
    "``` bash\n",
    "# (These example calls require you complete the LeNet / MNIST example first.)\n",
    "# time LeNet training on CPU for 10 iterations\n",
    "caffe time -model examples/mnist/lenet_train_test.prototxt -iterations 10\n",
    "# time LeNet training on GPU for the default 50 iterations\n",
    "caffe time -model examples/mnist/lenet_train_test.prototxt -gpu 0\n",
    "# time a model architecture with the given weights on the first GPU for 10 iterations\n",
    "caffe time -model examples/mnist/lenet_train_test.prototxt -weights examples/mnist/lenet_iter_10000.caffemodel -gpu 0 -iterations 10\n",
    "```\n",
    "\n",
    "#### Python\n",
    "- The Python interface – `pycaffe` – is the caffe module and its scripts in caffe/python. \n",
    "    - caffe.Net is the central interface for loading, configuring, and running models. caffe.Classifier and caffe.Detector provide convenience interfaces for common tasks.\n",
    "    - caffe.SGDSolver exposes the solving interface.\n",
    "    - caffe.io handles input / output with preprocessing and protocol buffers.\n",
    "    - caffe.draw visualizes network architectures.\n",
    "    - Caffe blobs are exposed as numpy ndarrays for ease-of-use and efficiency.\n",
    "\n",
    "### MATLAB, ...\n",
    "- The MATLAB interface – matcaffe – is the caffe package in caffe/matlab in which you can integrate Caffe in your Matlab code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "### Image Classification and Filter Visualization\n",
    "- [Classification: Instant Recognition with Caffe](http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb)\n",
    "\n",
    "### Learning LeNet\n",
    "- [Solving in Python with LeNet](http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/01-learning-lenet.ipynb)\n",
    "\n",
    "### Editing model parameters\n",
    "- [Net Surgery](http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/net_surgery.ipynb)\n",
    "\n",
    "### CIFAR tutorial\n",
    "- [Alex’s CIFAR-10 tutorial, Caffe style](http://caffe.berkeleyvision.org/gathered/examples/cifar10.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others, ...\n",
    "### Install\n",
    "### Config\n",
    "### Tools\n",
    "### [Caffe Model Zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo)\n",
    "- [Doc](http://caffe.berkeleyvision.org/model_zoo.html)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
